<!DOCTYPE HTML>
<html>
	<head>
		<script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
        <script type="text/javascript">

            $( document ).ready(function() {
                $("#header").load("html/left_bar.html");
                $("#footer").load("html/footer.html");
                $("#script").load("html/ref.html");
            });

        </script>
    
        <!-- Script -->
        <div id='script'></div>  
	</head>
	<body id="top" style="visibility:hidden">

		<!-- Header -->
		<header id="header">
				
		</header>

		<!-- Main -->
			<div id="main">
                <section id="project">
                    <h1><b>Research Projects</b></h1>
                    <!-- DRL-Frameworks-->

                    <!--  DRL Framework for Unknown Social Networks-->
                    <h2><b>Deep Reinforcement Learning Meets Network Embedding for Evolving Social Networks</b></h2>
                    <article class="12u 12u$(12)">
                        <p style="margin: -20px 0 1em 0">
                            <a href="" class="far fa-sticky-note"><span class="label">Paper</span></a>
                            <a href="https://github.com/khurshedmemon/DQN-ESN" class="fab fa-github"><span class="label">Github</span></a>
                        </p>
                        <p><img class="image right img_size" src="images/drl_esn.png" />Competitive Influence Maximization (CIM) aims to maximize the influence of a party given the competition from other parties in the same social network, like companies find key users to promote their competitive products on the social network to achieve maximum profit. Recently, learning-based solutions are introduced to tackle the competitive influence maximization problem. However, such studies focus on the static nature of social networks. This paper proposes a deep reinforcement learning-based framework employing network embedding, termed as DRL-EMB, to tackle the CIM problem on evolving social networks. The DRL-EMB key objective is to find the best strategy to maximize the party's reward, considering budget and competition with information propagation and network evolving being run in parallel. We validate our proposed framework with the DRL-based model using hand-crafted state features (DRL-HCF) and heuristic-based methods. Experimental results show that our proposed framework, DRL-EMB, achieves better results than heuristic-based and DRL-HCF models while significantly outperforming the DRL-HCF model in terms of time efficiency. 
                    </article>
                    <hr>

                    <!-- DRL Framework for Unknown Social Networks -->
                    <h2><b>Deep Reinforcement Learning and Transfer Learning for Unknown Social Networks</b></h2>
                    <article class="12u 12u$(12)">
                        <p style="margin: -20px 0 1em 0">
                            <a href="http://dx.doi.org/10.1109/ASONAM49781.2020.9381471" class="far fa-sticky-note"><span class="label">Paper</span></a>                            
                            <a href="https://github.com/khurshedmemon/DQN-UN-TL" class="fab fa-github"><span class="label">Github</span></a>
                        </p>
                        <p><img class="image right img_size" src="images/drl_un.png" />Recent studies have considered the reinforcement and deep reinforcement learning models to address the competitive influence maximization (CIM) problem. However, these models assume complete network topology information is available to address the CIM problem. This assumption is unrealistic as it is difficult to obtain complete social network data and requires exhaustive efforts to obtain it. In this work, we propose a deep reinforcement learning-based (DRL) model to tackle the competitive influence maximization on unknown social networks. Our proposed model has a two-fold objective: the first is to identify the time when to explore the network to collect network information. The second is to determine key influential users from the explored network, using optimal seed-selection strategy considering the competition in the social network. Moreover, we integrate the transfer learning in DRL to improve the training efficiency of DRL models. Experimental results show that our proposed DRL and transfer learning-based DRL models achieve significantly better performance than heuristic-based methods. 
                    </article>
                    <hr>

                    <!-- DRL Framework for Social Networks -->
                    <h2><b>DRL-based Method for Social Networks Analysis</b></h2>
                    <article class="12u 12u$(12)">
                        <p style="margin: -20px 0 1em 0">
                            <a href="https://www.mlgworkshop.org/2019/papers/MLG2019_paper_3.pdf" class="far fa-sticky-note"><span class="label">Paper</span></a>
                            <a href="https://github.com/khurshedmemon/dqn-kdd/" class="fab fa-github"><span class="label">Github</span></a>                            
                        </p>Competitive Influence Maximization (CIM) problem studies the competition among multiple parties where each party aims to maximize their profit while competing against other parties. Recently, Reinforcement-Learning based models have been proposed to address the CIM problem. However, such models are unscalable and incapable of handling changes in the network structure. Motivated by the recent success of Deep Reinforcement Learning models and their capability to handle complex problems, we propose a novel Deep Reinforcement learning based framework (DRIM) to address the multi-round competitive influence maximization problem. DRIM framework considers the community structure of the social network for budget allocation and feature extraction with deep Q network in order to reduce the computational time of seed selection. The proposed framework employs the quota-based $\epsilon$-greedy policy to explore the optimality of influence maximization strategies and budget allocation for each community. Experimental results show that the proposed DRIM framework performs better than the state-of-art algorithms to tackle the multi-round CIM problem.                          
                    </article>
                    <hr>

                    <!-- Novel Nested Q-Learning -->
                    <h2><b>Novel Nested Q-Learning for Time-Constrained Social Networks</b></h2>
                    <article class="12u 12u$(12)">
                        <p style="margin: -20px 0 1em 0">
                            <a href="https://ieeexplore.ieee.org/document/8584421" class="far fa-sticky-note"><span class="label">Paper</span></a>
                            <a href="https://github.com/khurshedmemon/Time-Constrained-CIM" class="fab fa-github"><span class="label">Github</span></a>
                        </p>
                        <p><img class="image right img_size" src="images/rl-nql.png"/>
                        In this paper, we examine a time-constrained competitive influence maximization where the parties wish to maximize their profits before the respective deadlines. Besides, the parties need to determine how to select the seed nodes and when to initiate information propagation in the network, such that the decision results in the optimal reward given the time and the budget constraint. To this end, we propose a novel reinforcement learning-based framework named seed-combination and seed-selection that is built on a nested Q-learning (NSQ) algorithm. This way, we can derive the optimal in both budget allocation and node selection that results in the maximum profit. In evaluating the proposed model, we consider the scenarios when the competitorsâ€™ strategy is known, unknown, and not available for training. The results show that the proposed NSQ algorithm could improve the rewards by up to 50% compared with the state-of-the-art algorithm, STORM-Q.
                        </p>                        
                    </article>
                    <hr>

                    <h2><b>Boosting Reinforcement Learning with Transfer Learning</b></h2>
                    <article class="12u 12u$(12)">
                        <p style="margin: -20px 0 1em 0">
                            <a href="https://www.computer.org/csdl/proceedings-article/wi/2018/732500a395/17D45Wuc38w" class="far fa-sticky-note"><span class="label">Paper</span></a>
                            <a href="https://github.com/khurshedmemon/Time-Constrained-CIM" class="fab fa-github"><span class="label">Github</span></a>                            
                        </p>

                        <!-- Boosting RL -->
                        <p>In this work, we propose a transfer learning method in reinforcement learning to reduce the training time and utilize the knowledge gained on source network to target network. Our method relies on two ideas, the first one is the state representation of the source and target networks in order to efficiently utilize the knowledge gained on source network to target network. The second idea is to transfer the final Q-solution of source network while learning on the target network. We validate our transfer learning method in similar or different settings of source and target networks while competing against the competitor's known strategies. Experimental results show that our proposed transfer learning method achieves similar or better performance as a baseline model while significantly reducing training time in all settings. 
                        </p>
                    </article>                    
                    <hr>
                </section>
            </div>
				
			
		<!-- Footer -->
			<footer id="footer">
				
			</footer>

	</body>
</html>
